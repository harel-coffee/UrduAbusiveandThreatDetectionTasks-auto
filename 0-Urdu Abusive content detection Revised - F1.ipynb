{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99ead2a",
   "metadata": {},
   "source": [
    "### Urdu Abusive content detection Revised - F1 - paper final\n",
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539cb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "#All the libraries that we need\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "#Common functions\n",
    "punc=['…','،','۔','،','۔','؟' , '.', ',']\n",
    "NFC = [('آ', 'آ'), ('ة', 'ۃ'), ('ـ', ''), ('ك', 'ک'), ('ه', 'ہ'), ('ۀ', 'ۂ'), \n",
    "       ('ى', 'ی'), ('ي', 'ی'), ('٠', '۰'), ('١', '۱'), ('٢', '۲'), \n",
    "       ('٣', '۳'), ('٤', '۴'), ('٥', '۵'), ('٦', '۶'), ('٧', '۷'), ('٨', '۸'), \n",
    "       ('٩', '۹'), ('ۓ', 'ئے'), ('ۓ', 'ئے'), ('ئ', 'ئی'),\n",
    "       (\"ِ\", \"\"), (\"ِ\", \"\"),(\"ُ\", \"\"),(\"َ\", \"\"),(\"ْ\", \"\"),(\"ٰ\", \"\"),(\"ً\", \"\"),(\"ّ\", \"\")      \n",
    "      ]\n",
    "\n",
    "# load file into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def read_as_dict_for_lemma(file):\n",
    "    text = load_file(file)\n",
    "    lines = text.split(\"\\n\")\n",
    "    wd = dict()\n",
    "    for line in lines:\n",
    "        toks = line.split(\":\")\n",
    "        if len(toks)==0 or len(toks)==1:\n",
    "            continue\n",
    "        wd[toks[0].strip()] = toks[1].strip()\n",
    "    return wd\n",
    "\n",
    "#Loading Lemmatizer\n",
    "dictionary = read_as_dict_for_lemma('Humayoun_lemmatizer.txt')\n",
    "\n",
    "def lemma(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def read_stopwords(file):\n",
    "    text = load_file(file)\n",
    "    tokens = text.split()\n",
    "    tokens = set(tokens)\n",
    "    return tokens\n",
    "\n",
    "def mkNFC(token):\n",
    "    for (ch1,ch2) in NFC:\n",
    "        token.replace(ch1,ch2)\n",
    "    return token\n",
    "\n",
    "# turn a txt into clean tokens\n",
    "def mk_tokens(txt, stw=None, lma=None, ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # split into tokens by white space\n",
    "    tokens = txt.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation+\"\".join(punc))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [mkNFC(token) for token in tokens]\n",
    "    if stw!=None:\n",
    "        # filter out stop words\n",
    "        stop_words = set(read_stopwords('stopword_Urmono.txt'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        tokens = [word for word in tokens if word!='کرنا']\n",
    "        tokens = [word for word in tokens if word!='پرنا']\n",
    "        tokens = [word for word in tokens if word!='رہنا']\n",
    "        tokens = [word for word in tokens if word!='جانا']\n",
    "        tokens = [word for word in tokens if word!='کَیا']\n",
    "    if lma!= None:\n",
    "        #apply lemmatization\n",
    "        tokens = [lemma(token, dictionary) for token in tokens]\n",
    "    \n",
    "    \n",
    "    #getting the char ngrams\n",
    "    charngrams=[]\n",
    "    if ch_how!=None:\n",
    "        charngrams = mkCNgram(tokens, ch_ngram, ch_how)\n",
    "    \n",
    "    wordngrams = mkNgram(tokens, ngram, how)\n",
    "    tokens = charngrams + wordngrams\n",
    "    \n",
    "    #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def mk_dict_from_dataset(txt, stw=None, lma=None, ngram=1, how=\"all\", \n",
    "                         ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    table = dict()\n",
    "    for line in txt.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid = cells[0].strip()\n",
    "        ctext = ' '.join(cells[1:])\n",
    "        tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "        table[cid] = tokens\n",
    "    return table\n",
    "\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab, stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # load file\n",
    "    file = load_file(filename)\n",
    "    # clean doc\n",
    "    #print(\"ch_ngram, ch_how:\", ch_ngram, ch_how)\n",
    "    table = mk_dict_from_dataset(file, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for cid in table:\n",
    "        tokens = table[cid]\n",
    "        vocab.update(tokens)\n",
    "\n",
    "def process_dataset(txt, vocab, train=True, stw=None, lma=None, \n",
    "                    ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    table = dict()\n",
    "    for line in txt.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid = cells[0].strip()\n",
    "        if (train):\n",
    "            ctext = ' '.join(cells[1:-1])\n",
    "            tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "            clabel = int(cells[2].strip())\n",
    "        else:\n",
    "            ctext = ' '.join(cells[1:])\n",
    "            tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "            clabel = None #for test\n",
    "        #print(clabel)\n",
    "        table[cid] = (tokens, clabel)\n",
    "        #            {id : (text,1/0)}\n",
    "    return table\n",
    "\n",
    "def extract_text_label_from_dict(table):\n",
    "    txts=list()\n",
    "    labels=list()\n",
    "    for item in table:\n",
    "        (txt, clabel) = table[item]\n",
    "        txts.append(txt)\n",
    "        labels.append( int(clabel) )\n",
    "    return txts, labels\n",
    "\n",
    "def extract_ids_from_dict(table):\n",
    "    return list(table.keys())\n",
    "\n",
    "def mkNgram(tokens, n=1, how=\"all\"):\n",
    "    if how==\"all\":\n",
    "        grams = list()\n",
    "        for r in range(n):\n",
    "            r+=1\n",
    "            grams.extend([\"\".join( tokens[i:i+r] ) for i in range(len(tokens)-r+1)]) \n",
    "        return grams\n",
    "    else:\n",
    "        return [\"\".join( tokens[i:i+n] ) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "\n",
    "#char ngrams\n",
    "def mkCNgram(tokens, L, cn_how=\"all\"):\n",
    "    tokens = \"\".join( tokens[:] )\n",
    "    tokens = mkNgram(tokens, L, cn_how)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def mk_test_splits(test_ans_slpit_csv, testcsv, vocab, stw=1, lma=1, \n",
    "                   ngram=3, how=\"all\",ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    test_x_public = list()\n",
    "    test_y_public = list()\n",
    "    test_x_private = list()\n",
    "    test_y_private = list()\n",
    "    \n",
    "    table = process_dataset(testcsv, vocab, False, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for line in test_ans_slpit_csv.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid   = cells[0].strip()\n",
    "        label = int( cells[1].strip() )\n",
    "        split = cells[2].strip()\n",
    "        if split=='public':\n",
    "            (tokens, _) = table[cid]\n",
    "            test_x_public.append( tokens )\n",
    "            test_y_public.append ( label )\n",
    "        else:\n",
    "            (tokens, _) = table[cid]\n",
    "            test_x_private.append( tokens )\n",
    "            test_y_private.append ( label )\n",
    "    return (test_x_public, test_y_public, test_x_private, test_y_private)\n",
    "        \n",
    "def mk_test(test_ans_slpit_csv, testcsv, vocab, stw=1, lma=1, \n",
    "                   ngram=3, how=\"all\",ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    test_x = list()\n",
    "    test_y = list()\n",
    "    \n",
    "    table = process_dataset(testcsv, vocab, False, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for line in test_ans_slpit_csv.splitlines():\n",
    "        #print(line)\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid   = cells[0].strip()\n",
    "        label = cells[1].strip()\n",
    "        split = cells[2].strip()\n",
    "        (tokens, _) = table[cid]\n",
    "        test_x.append( tokens )\n",
    "        test_y.append ( int(label) )\n",
    "    \n",
    "    return (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8f8e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "Counter({0: 1213, 1: 1187})\n",
      "1100\n",
      "Counter({1: 563, 0: 537})\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduAbusive/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduAbusive/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "\n",
    "                            \n",
    "table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(len(y))\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "testcsv = load_file('UrduAbusive/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                           stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                           ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "print(len(test_y))\n",
    "counter = Counter(test_y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a99466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "Most common: [('کرنا', 779), ('حرام', 352)]\n",
      "Vocabulary Size: 5695\n",
      "A sample doc and its y: ['قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام'] 1\n"
     ]
    }
   ],
   "source": [
    "v_stw=1 \n",
    "v_lma=1\n",
    "v_ngram=1\n",
    "v_how = \"all\"\n",
    "v_mode=\"freq\"\n",
    "v_ch_ngram=2\n",
    "v_ch_how=None\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "t0 = time.clock()\n",
    "print()\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduAbusive/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduAbusive/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "keys = extract_ids_from_dict(table)\n",
    "#print(keys[0]) \n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode=v_mode)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vector_size = len(tokenizer.word_index) + 1\n",
    "#print(\"sentence vector size:\", vector_size)\n",
    "                            \n",
    "\n",
    "train_x = X\n",
    "train_y = array(y)\n",
    "n_words = train_x.shape[1]\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "testcsv = load_file('UrduAbusive/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                           stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                           ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "\n",
    "#print(test_x[])\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(test_x, mode=v_mode)\n",
    "#print(Xtest)\n",
    "                        \n",
    "\n",
    "#models\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "\n",
    "\n",
    "mnames = list()\n",
    "mnames.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "mnames.append(\"svm.SVC(probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(train_x,train_y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    \n",
    "    print(test_pred_labels[0:13])\n",
    "    print(\"====\")\n",
    "    print(test_y[0:13])\n",
    "\n",
    "    predictions_labels=test_pred_labels\n",
    "    #predictions_labels = list()\n",
    "    #for v in test_pred_labels:\n",
    "    #    predictions_labels.append( int(v) )\n",
    "\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "    \n",
    "   \n",
    "\n",
    "    t1 = time.clock() - t0\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+str(v_mode)+\",\"+mnames[idx]+\",\"+str(len(vocab))+\",\"+str(vector_size)+\",\"+str(t1)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28deed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f7c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01eb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f6008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f8e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa3c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228db8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db2389f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple MLP model to predict the sentiment of encoded reviews.\n",
    "#f1 around ~78 max\n",
    "def mkMLPExp(opt_stopwords, opt_lemma, opt_ngram, opt_how_ngram, opt_mode, ch_ngram, ch_how):\n",
    "    print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "    for v_stw in opt_stopwords:\n",
    "        for v_lma in opt_lemma:\n",
    "            for v_ngram in opt_ngram:\n",
    "                for v_how_ngram in opt_how_ngram:\n",
    "                    \n",
    "                    for v_mode in opt_mode:\n",
    "                        model=\"MLP\"\n",
    "                        t0 = time.clock()\n",
    "                        print()\n",
    "                        print()\n",
    "                        # define vocab\n",
    "                        vocab = Counter()\n",
    "                        # add all text to vocab\n",
    "                        add_doc_to_vocab('UrduAbusive/train.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                                             ngram=v_ngram, how=v_how_ngram, \n",
    "                                                 ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "                        add_doc_to_vocab('UrduAbusive/test.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                                             ngram=v_ngram, how=v_how_ngram, \n",
    "                                                 ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "                            \n",
    "                        print(\"Most common:\", vocab.most_common(2))\n",
    "                        vocab = set(vocab)\n",
    "                        print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "                        table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, \n",
    "                                                    train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how_ngram,\n",
    "                                                     ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "\n",
    "                        X_docs, y = extract_text_label_from_dict(table)\n",
    "                        print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "                        keys = extract_ids_from_dict(table)\n",
    "                        \n",
    "                        # create the tokenizer\n",
    "                        tokenizer = Tokenizer()\n",
    "                        # fit the tokenizer on the documents\n",
    "                        tokenizer.fit_on_texts(X_docs)\n",
    "                        X = tokenizer.texts_to_matrix(X_docs, mode=v_mode)\n",
    "\n",
    "                        # define vocabulary size (largest integer value)\n",
    "                        vector_size = len(tokenizer.word_index) + 1\n",
    "                        #print(\"sentence vector size:\", vector_size)\n",
    "                            \n",
    "\n",
    "                        train_x = X\n",
    "                        train_y = array(y)\n",
    "\n",
    "                        \n",
    "                        n_words = train_x.shape[1]\n",
    "\n",
    "                        # define network\n",
    "                        model = Sequential()\n",
    "                        model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "                        model.add(Dense(25, input_shape=(n_words,), activation='relu'))\n",
    "                        model.add(Dense(1, activation='sigmoid'))\n",
    "                        # compile network\n",
    "                        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                        # fit network\n",
    "                        model.fit(train_x, train_y, epochs=50, verbose=0)\n",
    "                        \n",
    "                        \n",
    "                        # load all the test comments from test.csv and split in public and private sets\n",
    "                        testcsv = load_file('UrduAbusive/test.csv')       \n",
    "                        test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "\n",
    "                        (test_x, test_y) = mk_test(\n",
    "                            test_ans_slpit_csv, testcsv, vocab, stw=v_stw, lma=v_lma, ngram=v_ngram, \n",
    "                                how=v_how_ngram, ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "\n",
    "                          # encode training data set\n",
    "                        Xtest = tokenizer.texts_to_matrix(test_x, mode=v_mode)\n",
    "                        test_y = array(test_y)\n",
    "                        \n",
    "                        # make probability predictions with the model\n",
    "                        predictions_scores = model.predict(Xtest)\n",
    "                        predictions_labels = model.predict_classes(Xtest)\n",
    "\n",
    "                        \n",
    "                        predictions_labels = predictions_labels[:, 0]\n",
    "                        f1 = f1_score(test_y, predictions_labels)\n",
    "\n",
    "                        strf1 = str(f1*100)\n",
    "                            \n",
    "                        t1 = time.clock() - t0\n",
    "                        msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how_ngram)+\",\"+str(v_mode)+\",Simple MLP,\"+str(len(vocab))+\",\"+str(vector_size)+\",\"+str(t1)+\",\"+strf1\n",
    "                            \n",
    "                        print(msg)\n",
    "                        print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b125e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,82.80178280000109,76.13741875580317\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,78.44287610000174,74.10281280310377\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,77.9492599999976,77.77777777777779\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,78.62643080000271,73.89162561576356\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,80.8361642000018,76.34011090573014\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,79.68517030000294,77.71929824561404\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,81.72883290000027,77.5768535262206\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,80.3107760999992,75.26265520534861\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,80.63414759999796,78.0399274047187\n",
      "=========================================================================\n",
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "\n",
      "Most common: [('نا', 4913), ('ان', 3101)]\n",
      "Vocabulary Size: 26730\n",
      "A sample doc and its y: ['قو', 'وم', 'مپ', 'پر', 'را', 'ان', 'نا', 'اخ', 'خا', 'ان', 'ند', 'دا', 'ان', 'نی', 'ین', 'نم', 'مک', 'کح', 'حر', 'را', 'ام', 'قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "1,1,2,all,freq,Simple MLP,26730,20485,72.74364219999916,78.04024496937882\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "opt_stopwords =[1] #[None, 1]\n",
    "opt_lemma =[1] #[None, 1]\n",
    "opt_ngram = [2] #[1,2,3]\n",
    "opt_how_ngram = [\"all\"] #\"exact\",\n",
    "opt_mode = [\"freq\"] #, \"count\", \"freq\", \"tfidf\"]\n",
    "\n",
    "for i in range(10):\n",
    "    mkMLPExp(opt_stopwords, opt_lemma, opt_ngram, opt_how_ngram, opt_mode, 2, \"exact\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7429fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aed63e29",
   "metadata": {},
   "source": [
    "# Word2Vec Results with and without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78b49cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "Most common: [('کرنا', 779), ('حرام', 352), ('کتا', 342), ('بکواس', 324), ('مادرچود', 292)]\n",
      "Vocabulary Size: 24949\n",
      "A sample doc and its y: ['قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام', 'قومپرانا', 'پراناخاندانی', 'خاندانینمک', 'نمکحرام'] 1\n",
      "len: 9\n",
      "0-> 1213\n",
      "1-> 1187\n",
      "1187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "v_stw = 1 #1 # None #\n",
    "v_lma = 1  \n",
    "v_ngram = 2\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = None\n",
    "\n",
    "url='Urdu-word2vec-implementations/samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "\n",
    "# define vocab  \n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduAbusive/train.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                 ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduAbusive/test.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(5))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs_train, y = extract_text_label_from_dict(table)\n",
    "ytrain = y\n",
    "\n",
    "print(\"A sample doc and its y:\", X_docs_train[0], y[0])\n",
    "print(\"len:\", len(X_docs_train[0]))\n",
    "\n",
    "zero=list()\n",
    "one=list()\n",
    "for ty in ytrain:\n",
    "    if ty==0:\n",
    "        zero.append(0)\n",
    "    elif ty==1:\n",
    "        one.append(1)\n",
    "    else:\n",
    "        print(ty)\n",
    "        \n",
    "print ( \"0->\", len(zero))\n",
    "print ( \"1->\", len(one)) \n",
    "print(len(X_docs_train)-len(zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8c09896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 300)\n",
      "total instances: 2400\n",
      "Counter({0: 1213, 1: 1187})\n",
      "(1100, 300)\n",
      "1,1,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=1, probability=True)24949,300,78.05325987144168\n",
      "=========================================================================\n",
      "1,1,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=2, probability=True)24949,300,79.12087912087912\n",
      "=========================================================================\n",
      "1,1,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=3, probability=True)24949,300,78.55153203342618\n",
      "=========================================================================\n",
      "1,1,2,all,,Word2Vec,svm.SVC(probability=True)24949,300,79.16287534121929\n",
      "=========================================================================\n",
      "1,1,2,all,,Word2Vec,svm.SVC(kernel='sigmoid', probability=True)24949,300,75.78558225508317\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean()\n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Abusive'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Abusive', axis = 1)\n",
    "train_y = docs_vectors['Abusive']\n",
    "\n",
    "print(\"total instances:\", len(train_x))\n",
    "##rint(len(train_y))\n",
    "\n",
    "#print(train_x)\n",
    "#print(train_y)\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "#oversample = SMOTE()\n",
    "#X, y = oversample.fit_resample(train_x, train_y)\n",
    "X=train_x\n",
    "y=train_y\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv \n",
    "testcsv = load_file('UrduAbusive/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, stw=v_stw, lma=v_lma, ngram=v_ngram, \n",
    "                              how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "            \n",
    "X_docs_test = test_x\n",
    "test_y = array(test_y)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "\n",
    "names = list()\n",
    "names.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "names.append(\"svm.SVC(probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    #print(test_pred_labels[0:3])\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for v in test_pred_labels:\n",
    "        predictions_labels.append( int(v) )\n",
    "\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "    vector_size=300\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+\",Word2Vec,\"+names[idx]+str(len(vocab))+\",\"+str(vector_size)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f12e046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d835799",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434deeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Deep CNN model to predict the sentiment of encoded reviews.\n",
    "#f1 around \n",
    "def mkCNNExp(opt_stopwords, opt_lemma, opt_ngram, opt_how_ngram, opt_mode, ch_ngram, ch_how):\n",
    "    print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "    for v_stw in opt_stopwords:\n",
    "        for v_lma in opt_lemma:\n",
    "            for v_ngram in opt_ngram:\n",
    "                for v_how_ngram in opt_how_ngram:\n",
    "                    \n",
    "                    for v_mode in opt_mode:\n",
    "                        t0 = time.clock()\n",
    "                        print()\n",
    "                        print()\n",
    "                        # define vocab\n",
    "                        vocab = Counter()\n",
    "                        # add all text to vocab\n",
    "                        add_doc_to_vocab('UrduAbusive/train.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                                             ngram=v_ngram, how=v_how_ngram, \n",
    "                                                 ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "                        add_doc_to_vocab('UrduAbusive/test.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                                             ngram=v_ngram, how=v_how_ngram, \n",
    "                                                 ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "                            \n",
    "                        print(\"Most common:\", vocab.most_common(2))\n",
    "                        vocab = set(vocab)\n",
    "                        print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "                        table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, \n",
    "                                                    train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how_ngram,\n",
    "                                                     ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "\n",
    "                        X_docs, y = extract_text_label_from_dict(table)\n",
    "                        print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "                        \n",
    "                        \n",
    "                        # create the tokenizer\n",
    "                        tokenizer = Tokenizer()\n",
    "                        # fit the tokenizer on the documents\n",
    "                        tokenizer.fit_on_texts(X_docs)\n",
    "                        # encode training data set\n",
    "                        encoded_docs = tokenizer.texts_to_sequences(X_docs)\n",
    "                        # pad sequences\n",
    "                        max_length = max([len(s) for s in X_docs])\n",
    "                        #print(max_length)\n",
    "\n",
    "                            \n",
    "                        Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "                        ytrain = array(y)\n",
    "\n",
    "                        vocab_size = len(tokenizer.word_index) + 1\n",
    "                        vector_size = vocab_size\n",
    "                        train_x = Xtrain\n",
    "                        train_y = ytrain\n",
    "                        \n",
    "                        n_words = train_x.shape[1]\n",
    "\n",
    "                        # define network\n",
    "                        model = Sequential()\n",
    "                        model.add(  Embedding(vocab_size, 100, input_length=max_length)  )\n",
    "                        model.add(  Conv1D(filters=32, kernel_size=8, activation='relu') )\n",
    "                        model.add(  MaxPooling1D(pool_size=2) )\n",
    "                        model.add(  Flatten())\n",
    "                        model.add(  Dense(10, activation='relu'))\n",
    "                        model.add(  Dense(1, activation='sigmoid'))\n",
    "                        print(model.summary())\n",
    "\n",
    "                        # compile network\n",
    "                        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                        # fit network\n",
    "                        model.fit(train_x, train_y, epochs=50, verbose=0)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # load all the test comments from test.csv and split in public and private sets\n",
    "                        testcsv = load_file('UrduAbusive/test.csv')       \n",
    "                        test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "\n",
    "                        (test_x, test_y) = mk_test(\n",
    "                            test_ans_slpit_csv, testcsv, vocab, stw=v_stw, lma=v_lma, ngram=v_ngram, \n",
    "                                how=v_how_ngram, ch_ngram=ch_ngram, ch_how=ch_how)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        X_docs_test = test_x\n",
    "                        # sequence encode\n",
    "                        encoded_docs_test = tokenizer.texts_to_sequences(X_docs_test)\n",
    "                        # pad sequences\n",
    "                        Xtest = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "                        test_y = array(test_y)\n",
    "                        \n",
    "                        \n",
    "                        # make probability predictions with the model\n",
    "                        predictions_scores = model.predict(Xtest)\n",
    "                        predictions_labels = model.predict_classes(Xtest)\n",
    "\n",
    "                        \n",
    "                        predictions_labels = predictions_labels[:, 0]\n",
    "                        f1 = f1_score(test_y, predictions_labels)\n",
    "\n",
    "                        strf1 = str(f1*100)\n",
    "                            \n",
    "                        t1 = time.clock() - t0\n",
    "                        msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"\n",
    "                        msg+= str(v_how_ngram)+\",\"+str(v_mode)+\",Deep CNN,\"\n",
    "                        msg+= str(len(vocab))+\",\"+str(vector_size)+\",\"\n",
    "                        msg+= str(t1)+\",\"+strf1\n",
    "                            \n",
    "                        print(msg)\n",
    "                        print(\"=========================================================================\")\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_stopwords =[1] #[None, 1]\n",
    "opt_lemma =[1] #[None, 1]\n",
    "opt_ngram = [3] #[1,2,3]\n",
    "opt_how_ngram = [\"all\"] #\"exact\",\n",
    "opt_mode = [\"binary\", \"freq\"] #, \"count\", \"freq\", \"tfidf\"]\n",
    "\n",
    "mkCNNExp(opt_stopwords, opt_lemma, opt_ngram, opt_how_ngram, opt_mode, 2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207f988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537c12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852ade06",
   "metadata": {},
   "source": [
    "# Develop an n-gram CNN Model for Sentiment Analysis\n",
    "### \"CNN Word2Vec NGram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f35ab3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Most common: [('کرنا', 779), ('حرام', 352)]\n",
      "Vocabulary Size: 5695\n",
      "A sample doc and its y: ['قوم', 'پرانا', 'خاندانی', 'نمک', 'حرام'] 1\n",
      "Max document length: 27\n",
      "Vocabulary size: 4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhumayoun\\Anaconda3\\envs\\mle\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.5344 - accuracy: 0.7204\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.2780 - accuracy: 0.8950\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1725 - accuracy: 0.9362\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1272 - accuracy: 0.9546\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 12s 5ms/step - loss: 0.1032 - accuracy: 0.9533\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 12s 5ms/step - loss: 0.0774 - accuracy: 0.9558\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 12s 5ms/step - loss: 0.0736 - accuracy: 0.9592\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 12s 5ms/step - loss: 0.0623 - accuracy: 0.9583\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0557 - accuracy: 0.9646\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0568 - accuracy: 0.9650\n",
      "1,1,1,all,NA,\"Very Deep CNN\",5695,4657,131.26398470000095,79.19227392449517\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "#N gram CNN model\n",
    "\n",
    "from numpy import array\n",
    "import numpy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s) for s in lines])\n",
    "\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length,  padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    \n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(32, 1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D()(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(32, 2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D()(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(32, 3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D()(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # channel 4\n",
    "    inputs4 = Input(shape=(length,))\n",
    "    embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "    conv4 = Conv1D(32, 4, activation='relu')(embedding4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling1D()(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3, flat4])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    #model.summary()\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model\n",
    "\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = None\n",
    "\n",
    "t0 = time.clock()\n",
    "print()\n",
    "print()\n",
    "\n",
    "# define vocab  \n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduAbusive/train.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                 ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduAbusive/test.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "table = process_dataset(load_file('UrduAbusive/train.csv'), vocab, train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "y = array(y)\n",
    "\n",
    "## create tokenizer\n",
    "tokenizer = create_tokenizer(X_docs)\n",
    "# calculate max document length\n",
    "length = max_length(X_docs)\n",
    "## load all training reviews\n",
    "X = encode_text(tokenizer, X_docs, length)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_x = X\n",
    "\n",
    "# calculate max document length\n",
    "length = max_length(train_x)\n",
    "print('Max document length: %d' % length)\n",
    "    \n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = train_x\n",
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX,trainX], y, epochs=10, batch_size=7)\n",
    "\n",
    "# load all the test comments from test.csv and split in public and private sets\n",
    "testcsv = load_file('UrduAbusive/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduAbusive/test_with_ans_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "                        \n",
    "Xtest = encode_text(tokenizer, test_x, length)\n",
    "test_y = array(test_y)\n",
    "\n",
    "# make probability predictions with the model\n",
    "predictions_scores = model.predict([Xtest,Xtest,Xtest,Xtest])\n",
    "\n",
    "\n",
    "predictions_labels = list()\n",
    "for score in predictions_scores:\n",
    "    if score>=0.5:\n",
    "        predictions_labels.append( int(1) )\n",
    "    else:\n",
    "        predictions_labels.append( int(0) )\n",
    "        \n",
    "f1 = f1_score(test_y, predictions_labels)\n",
    "\n",
    "strf1 = str(f1*100)\n",
    "\n",
    "t1 = time.clock() - t0\n",
    "\n",
    "msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"\n",
    "msg+= str(v_how)+\",\"+\"NA\"+\",\\\"Very Deep CNN\\\",\"\n",
    "msg+= str(len(vocab))+\",\"+str(vocab_size)+\",\"\n",
    "msg+= str(t1)+\",\"+strf1\n",
    "                            \n",
    "print(msg)\n",
    "print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde6903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b304b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc436a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d728c88",
   "metadata": {},
   "source": [
    "## Second Submission: Bag of words SVM segmoid\n",
    "#### UrduAbusive_results_1.csv F1:0.80548\tROC_AUCC: 0.82061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29853cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "punc=['…','،','۔','،','۔','؟' , '.', ',']\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrdu.txt'\n",
    "#vocab_filename = 'vocabUrduAbusive_no_stw_removal_allwords.txt' \n",
    "#UrduAbusive_results_1_1.csv\n",
    "#F1 0.77871 AUC: 0.79999\n",
    "\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduAbusive/train.csv'), vocab)\n",
    "\n",
    "X_docs = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode='freq')\n",
    "y = array(extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1))\n",
    "\n",
    "print(len(X[0]))\n",
    "      \n",
    "\n",
    "train_x = X\n",
    "train_y = y\n",
    "\n",
    "model = svm.SVC(kernel='sigmoid', probability=True).fit(train_x, train_y)\n",
    "\n",
    "\n",
    "#f1 = f1_score(test_y, test_pred)\n",
    "#res_SVM_segmoid_f1.append(f1*100)\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduAbusive/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(X_docs_test, mode='freq')\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "       \n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score(model, X, y, cv=5, scoring='f1')\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+\"{:.5f}\".format(test_pred_scores[i])+'\\n'\n",
    "\n",
    "#file = open('UrduAbusive/results/UrduAbusive_results_1_1.csv', 'w', encoding='utf8')\n",
    "#file.write(results)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f75d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop Super Learner Ensembles in Python\n",
    "# Classification for abusive language problem\n",
    "# Classification\n",
    "# example of a super learner using the mlens library\n",
    "# F1 0.77528 AUCC 0.5\n",
    "\n",
    "# it did not perform on Absuive dataset\n",
    "# may be add better classifers\n",
    "#https://machinelearningmastery.com/super-learner-ensemble-in-python/\n",
    "#How to make it better?\n",
    "# use only the best performing algos\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(LogisticRegression(solver='liblinear'))\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    models.append(SVC(gamma='scale', probability=True))\n",
    "    #models.append(svm.SVC(kernel='sigmoid'))\n",
    "    #models.append(svm.SVC(), probability=True)\n",
    "    models.append(GaussianNB())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(AdaBoostClassifier())\n",
    "    models.append(BaggingClassifier(n_estimators=10))\n",
    "    models.append(RandomForestClassifier(n_estimators=10))\n",
    "    models.append(ExtraTreesClassifier(n_estimators=10))\n",
    "    return models\n",
    " \n",
    "# create the super learner\n",
    "def get_super_learner(X):\n",
    "    ensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X))\n",
    "    # add base models\n",
    "    models = get_models()\n",
    "    ensemble.add(models)\n",
    "    # add the meta model\n",
    "    ensemble.add_meta(LogisticRegression(solver='lbfgs'))\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrdu.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduAbusive/train.csv'), vocab)\n",
    "\n",
    "X_docs = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode='freq')\n",
    "y = array(extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1))\n",
    "\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# create the super learner\n",
    "ensemble = get_super_learner(X)\n",
    "# fit the super learner\n",
    "ensemble.fit(X, y)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# make predictions on hold out set\n",
    "yhat = ensemble.predict(X_val)\n",
    "print(yhat[0:10])\n",
    "print('F1 Super Learner: %.3f' % (f1_score(y_val, yhat) * 100))\n",
    "\n",
    "#accuracy Super Learner: 83.000\n",
    "#F1 Super Learner: 83.406\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduAbusive/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(X_docs_test, mode='freq')\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = ensemble.predict(Xtest)\n",
    "#test_pred_scr = ensemble.predict_probb(Xtest)\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_labels)):\n",
    "    results += str(ids[i])+\",\"+str(int(test_pred_labels[i]))+\",\"+\"0.0\"+'\\n'\n",
    "\n",
    "file = open('UrduAbusive/results/UrduAbusive_results_4.csv', 'w', encoding='utf8')\n",
    "file.write(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b233d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b68ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting based ensenble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200c64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc32275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrdu.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduAbusive/train.csv'), vocab)\n",
    "\n",
    "X_docs = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "\n",
    "X_docs = trim_docs(X_docs, 4)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode='freq')\n",
    "y = array(extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1))\n",
    "\n",
    "res_SVM_segmoid_f1=[]\n",
    "\n",
    "train_x = X\n",
    "train_y = y\n",
    "\n",
    "model = svm.SVC(kernel='sigmoid', probability=True).fit(train_x, train_y)\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduAbusive/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "X_docs_test = trim_docs(X_docs_test, 4)\n",
    "\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(X_docs_test, mode='freq')\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "       \n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+\"{:.5f}\".format(test_pred_scores[i])+'\\n'\n",
    "\n",
    "file = open('UrduAbusive/results/UrduAbusive_results_3.csv', 'w', encoding='utf8')\n",
    "file.write(results)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36467667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result got worse with SMOTE\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrdu.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduAbusive/train.csv'), vocab)\n",
    "\n",
    "X_docs = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode='freq')\n",
    "y = array(extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1))\n",
    "\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "res_SVM_segmoid_f1=[]\n",
    "\n",
    "train_x = X\n",
    "train_y = y\n",
    "\n",
    "model = svm.SVC(kernel='sigmoid', probability=True).fit(train_x, train_y)\n",
    "\n",
    "\n",
    "#f1 = f1_score(test_y, test_pred)\n",
    "#res_SVM_segmoid_f1.append(f1*100)\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduAbusive/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(X_docs_test, mode='freq')\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "       \n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+\"{:.5f}\".format(test_pred_scores[i])+'\\n'\n",
    "\n",
    "file = open('UrduAbusive/results/UrduAbusive_results_2.csv', 'w', encoding='utf8')\n",
    "file.write(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54769726",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aba123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduAbusive/train.csv'), vocab)\n",
    "\n",
    "X_docs = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "X_docs_trim = trim_docs(X_docs, 3)\n",
    "X_docs_trim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd19d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrdu.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a9448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
