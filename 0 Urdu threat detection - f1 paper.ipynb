{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a93fb1d",
   "metadata": {},
   "source": [
    "# Urdu threat detection final paper\n",
    "Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ca5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "#All the libraries that we need\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "\n",
    "#Common functions\n",
    "punc=['…','،','۔','،','۔','؟' , '.', ',']\n",
    "NFC = [('آ', 'آ'), ('ة', 'ۃ'), ('ـ', ''), ('ك', 'ک'), ('ه', 'ہ'), ('ۀ', 'ۂ'), \n",
    "       ('ى', 'ی'), ('ي', 'ی'), ('٠', '۰'), ('١', '۱'), ('٢', '۲'), \n",
    "       ('٣', '۳'), ('٤', '۴'), ('٥', '۵'), ('٦', '۶'), ('٧', '۷'), ('٨', '۸'), \n",
    "       ('٩', '۹'), ('ۓ', 'ئے'), ('ۓ', 'ئے'), ('ئ', 'ئی'),\n",
    "       (\"ِ\", \"\"), (\"ِ\", \"\"),(\"ُ\", \"\"),(\"َ\", \"\"),(\"ْ\", \"\"),(\"ٰ\", \"\"),(\"ً\", \"\"),(\"ّ\", \"\")      \n",
    "      ]\n",
    "\n",
    "# load file into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def read_as_dict_for_lemma(file):\n",
    "    text = load_file(file)\n",
    "    lines = text.split(\"\\n\")\n",
    "    wd = dict()\n",
    "    for line in lines:\n",
    "        toks = line.split(\":\")\n",
    "        if len(toks)==0 or len(toks)==1:\n",
    "            continue\n",
    "        wd[toks[0].strip()] = toks[1].strip()\n",
    "    return wd\n",
    "\n",
    "#Loading Lemmatizer\n",
    "dictionary = read_as_dict_for_lemma('Humayoun_lemmatizer.txt')\n",
    "\n",
    "def lemma(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def read_stopwords(file):\n",
    "    text = load_file(file)\n",
    "    tokens = text.split()\n",
    "    tokens = set(tokens)\n",
    "    return tokens\n",
    "\n",
    "def mkNFC(token):\n",
    "    for (ch1,ch2) in NFC:\n",
    "        token.replace(ch1,ch2)\n",
    "    return token\n",
    "\n",
    "# turn a txt into clean tokens\n",
    "def mk_tokens(txt, stw=None, lma=None, ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # split into tokens by white space\n",
    "    tokens = txt.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation+\"\".join(punc))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [mkNFC(token) for token in tokens]\n",
    "    if stw!=None:\n",
    "        # filter out stop words\n",
    "        stop_words = set(read_stopwords('stopword_Urmono.txt'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        #tokens = [word for word in tokens if len(word) > 1]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "        tokens = [word for word in tokens if word!='کرنا']\n",
    "        tokens = [word for word in tokens if word!='پرنا']\n",
    "        tokens = [word for word in tokens if word!='رہنا']\n",
    "        tokens = [word for word in tokens if word!='جانا']\n",
    "        tokens = [word for word in tokens if word!='کَیا']\n",
    "    if lma!= None:\n",
    "        #apply lemmatization\n",
    "        tokens = [lemma(token, dictionary) for token in tokens]\n",
    "    \n",
    "    \n",
    "    #getting the char ngrams\n",
    "    charngrams=[]\n",
    "    if ch_how!=None:\n",
    "        charngrams = mkCNgram(tokens, ch_ngram, ch_how)\n",
    "    \n",
    "    wordngrams = mkNgram(tokens, ngram, how)\n",
    "    tokens = charngrams + wordngrams\n",
    "    \n",
    "    #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def mk_dict_from_dataset(txt, stw=None, lma=None, ngram=1, how=\"all\", \n",
    "                         ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    table = dict()\n",
    "    for line in txt.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid = cells[0].strip()\n",
    "        ctext = ' '.join(cells[1:])\n",
    "        tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "        table[cid] = tokens\n",
    "    return table\n",
    "\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab, stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # load file\n",
    "    file = load_file(filename)\n",
    "    # clean doc\n",
    "    #print(\"ch_ngram, ch_how:\", ch_ngram, ch_how)\n",
    "    table = mk_dict_from_dataset(file, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for cid in table:\n",
    "        tokens = table[cid]\n",
    "        vocab.update(tokens)\n",
    "\n",
    "def process_dataset(txt, vocab, train=True, stw=None, lma=None, \n",
    "                    ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    table = dict()\n",
    "    for line in txt.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid = cells[0].strip()\n",
    "        if (train):\n",
    "            ctext = ' '.join(cells[1:-1])\n",
    "            tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "            clabel = cells[-1].strip()\n",
    "        else:\n",
    "            ctext = ' '.join(cells[1:])\n",
    "            tokens = mk_tokens(ctext, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "            clabel = None #for test\n",
    "        #print(clabel)\n",
    "        table[cid] = (tokens, clabel)\n",
    "        #            {id : (text,1/0)}\n",
    "    return table\n",
    "\n",
    "def extract_text_label_from_dict(table):\n",
    "    txts=list()\n",
    "    labels=list()\n",
    "    for item in table:\n",
    "        (txt, clabel) = table[item]\n",
    "        txts.append(txt)\n",
    "        labels.append(clabel)\n",
    "    return txts, labels\n",
    "\n",
    "def extract_ids_from_dict(table):\n",
    "    return list(table.keys())\n",
    "\n",
    "def mkNgram(tokens, n=1, how=\"all\"):\n",
    "    if how==\"all\":\n",
    "        grams = list()\n",
    "        for r in range(n):\n",
    "            r+=1\n",
    "            grams.extend([\"_\".join( tokens[i:i+r] ) for i in range(len(tokens)-r+1)]) \n",
    "        return grams\n",
    "    else:\n",
    "        return [\"_\".join( tokens[i:i+n] ) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "\n",
    "#char ngrams\n",
    "def mkCNgram(tokens, L, cn_how=\"all\"):\n",
    "    tokens = \"\".join( tokens[:] )\n",
    "    tokens = mkNgram(tokens, L, cn_how)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def mk_test_splits(test_ans_slpit_csv, testcsv, vocab, stw=1, lma=1, \n",
    "                   ngram=3, how=\"all\",ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    test_x_public = list()\n",
    "    test_y_public = list()\n",
    "    test_x_private = list()\n",
    "    test_y_private = list()\n",
    "    \n",
    "    table = process_dataset(testcsv, vocab, False, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for line in test_ans_slpit_csv.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid   = cells[0].strip()\n",
    "        label = cells[1].strip()\n",
    "        split = cells[2].strip()\n",
    "        if split=='public':\n",
    "            (tokens, _) = table[cid]\n",
    "            test_x_public.append( tokens )\n",
    "            test_y_public.append ( label )\n",
    "        else:\n",
    "            (tokens, _) = table[cid]\n",
    "            test_x_private.append( tokens )\n",
    "            test_y_private.append ( label )\n",
    "    return (test_x_public, test_y_public, test_x_private, test_y_private)\n",
    "        \n",
    "def mk_test(test_ans_slpit_csv, testcsv, vocab, stw=1, lma=1, \n",
    "                   ngram=3, how=\"all\",ch_ngram=2, ch_how=None):\n",
    "    c=1\n",
    "    test_x = list()\n",
    "    test_y = list()\n",
    "    \n",
    "    table = process_dataset(testcsv, vocab, False, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    for line in test_ans_slpit_csv.splitlines():\n",
    "        if c==1:\n",
    "            c=0\n",
    "            continue\n",
    "        cells = line.strip().split(\",\")\n",
    "        cid   = cells[0].strip()\n",
    "        label = cells[1].strip()\n",
    "        split = cells[2].strip()\n",
    "        (tokens, _) = table[cid]\n",
    "        test_x.append( tokens )\n",
    "        test_y.append ( int(label) )\n",
    "    return (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0512e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "Most common: [('کشمیر', 964), ('پاکستان', 874), ('انڈیا', 481), ('بھارت', 420), ('مودی', 383)]\n",
      "Vocabulary Size: 58325\n",
      "A sample doc and its y: ['لوئر', 'تھانہ', 'منڈہ', 'حدود', 'گوسم', 'فائرنگ', 'پانچ', 'افراد', 'افراد', 'تعلق', 'خاندان', 'لوئر_تھانہ', 'تھانہ_منڈہ', 'منڈہ_حدود', 'حدود_گوسم', 'گوسم_فائرنگ', 'فائرنگ_پانچ', 'پانچ_افراد', 'افراد_افراد', 'افراد_تعلق', 'تعلق_خاندان'] 0\n",
      "len: 21\n",
      "0-> 4929\n",
      "1-> 1071\n",
      "1071\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "v_stw = 1 #1 # None #\n",
    "v_lma = None  \n",
    "v_ngram = 2\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = None\n",
    "\n",
    "url='Urdu-word2vec-implementations/samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "\n",
    "# define vocab  \n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduThreat/train.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                 ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduThreat/test.csv', vocab, stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(5))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "table = process_dataset(load_file('UrduThreat/train.csv'), vocab, train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs_train, y = extract_text_label_from_dict(table)\n",
    "ytrain = y\n",
    "\n",
    "print(\"A sample doc and its y:\", X_docs_train[0], y[0])\n",
    "print(\"len:\", len(X_docs_train[0]))\n",
    "\n",
    "zero=list()\n",
    "one=list()\n",
    "for ty in ytrain:\n",
    "    if ty==\"0\":\n",
    "        zero.append(0)\n",
    "    elif ty=='1':\n",
    "        one.append(1)\n",
    "    else:\n",
    "        print(ty)\n",
    "        \n",
    "print ( \"0->\", len(zero))\n",
    "print ( \"1->\", len(one)) \n",
    "print(len(X_docs_train)-len(zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1ea5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 300)\n",
      "total instances: 6000\n",
      "Counter({'0': 4929, '1': 4929})\n",
      "(3950, 300)\n",
      "1,None,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=1, probability=True)58325,300,42.61036468330135\n",
      "=========================================================================\n",
      "1,None,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=2, probability=True)58325,300,44.61883408071749\n",
      "=========================================================================\n",
      "1,None,2,all,,Word2Vec,svm.SVC(kernel='poly',degree=3, probability=True)58325,300,43.02884615384615\n",
      "=========================================================================\n",
      "1,None,2,all,,Word2Vec,svm.SVC(probability=True)58325,300,43.14720812182742\n",
      "=========================================================================\n",
      "1,None,2,all,,Word2Vec,svm.SVC(kernel='sigmoid', probability=True)58325,300,35.860409145607704\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean()\n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']\n",
    "\n",
    "print(\"total instances:\", len(train_x))\n",
    "##rint(len(train_y))\n",
    "\n",
    "#print(train_x)\n",
    "#print(train_y)\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv \n",
    "testcsv = load_file('UrduThreat/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduThreat/test_answ_and_split.csv')\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, stw=v_stw, lma=v_lma, ngram=v_ngram, \n",
    "                              how=v_how, ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "            \n",
    "X_docs_test = test_x\n",
    "test_y = array(test_y)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "\n",
    "names = list()\n",
    "names.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "names.append(\"svm.SVC(probability=True)\")\n",
    "names.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    #print(test_pred_labels[0:3])\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for v in test_pred_labels:\n",
    "        predictions_labels.append( int(v) )\n",
    "\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "    vector_size=300\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+\",Word2Vec,\"+names[idx]+str(len(vocab))+\",\"+str(vector_size)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d74c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51829a0",
   "metadata": {},
   "source": [
    "# Applying SMOTE on bag of words model with normal models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14087a3",
   "metadata": {},
   "source": [
    "### To start with Applying bag of words model with normal models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start with Applying bag of words model with normal models\n",
    "v_stw=1 \n",
    "v_lma=1\n",
    "v_ngram=1\n",
    "v_how = \"all\"\n",
    "v_mode=\"freq\"\n",
    "v_ch_ngram=2\n",
    "v_ch_how=None\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "t0 = time.clock()\n",
    "print()\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduThreat/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduThreat/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "table = process_dataset(load_file('UrduThreat/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "keys = extract_ids_from_dict(table)\n",
    "#print(keys[0]) \n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode=v_mode)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vector_size = len(tokenizer.word_index) + 1\n",
    "#print(\"sentence vector size:\", vector_size)\n",
    "                            \n",
    "\n",
    "train_x = X\n",
    "train_y = array(y)\n",
    "n_words = train_x.shape[1]\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "testcsv = load_file('UrduThreat/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduThreat/test_answ_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                           stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                           ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "\n",
    "#print(test_x[])\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(test_x, mode=v_mode)\n",
    "#print(Xtest)\n",
    "\n",
    "\n",
    "#models\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "\n",
    "mnames = list()\n",
    "mnames.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "mnames.append(\"svm.SVC(probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(train_x,train_y)\n",
    "    print(\"training completed...\")\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    \n",
    "    print(test_pred_labels[0:13])\n",
    "    print(\"====\")\n",
    "    print(test_y[0:13])\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for v in test_pred_labels:\n",
    "        predictions_labels.append( int(v) )\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "   \n",
    "    t1 = time.clock() - t0\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+str(v_mode)+\",\"+\n",
    "            mnames[idx]+\",\"+str(len(vocab))+\",\"+str(vector_size)+\",\"+str(t1)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f365d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d932aa6",
   "metadata": {},
   "source": [
    "## using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "Most common: [('کشمیر', 964), ('پاکستان', 874)]\n",
      "Vocabulary Size: 58325\n",
      "A sample doc and its y: ['لوئر', 'تھانہ', 'منڈہ', 'حدود', 'گوسم', 'فائرنگ', 'پانچ', 'افراد', 'افراد', 'تعلق', 'خاندان', 'لوئر_تھانہ', 'تھانہ_منڈہ', 'منڈہ_حدود', 'حدود_گوسم', 'گوسم_فائرنگ', 'فائرنگ_پانچ', 'پانچ_افراد', 'افراد_افراد', 'افراد_تعلق', 'تعلق_خاندان'] 0\n",
      "Counter({'0': 4929, '1': 4929})\n"
     ]
    }
   ],
   "source": [
    "#To start with Applying bag of words model with normal models\n",
    "v_stw = 1 #1 # None #\n",
    "v_lma = None  \n",
    "v_ngram = 2\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = None\n",
    "v_mode=\"freq\"\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "t0 = time.clock()\n",
    "print()\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduThreat/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduThreat/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "table = process_dataset(load_file('UrduThreat/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "keys = extract_ids_from_dict(table)\n",
    "#print(keys[0]) \n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode=v_mode)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vector_size = len(tokenizer.word_index) + 1\n",
    "#print(\"sentence vector size:\", vector_size)\n",
    "                            \n",
    "\n",
    "train_x = X\n",
    "train_y = array(y)\n",
    "n_words = train_x.shape[1]\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "testcsv = load_file('UrduThreat/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduThreat/test_answ_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                           stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                           ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "\n",
    "#print(test_x[])\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(test_x, mode=v_mode)\n",
    "#print(Xtest)\n",
    "\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#models\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(AdaBoostClassifier())\n",
    "#models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "#models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "\n",
    "\n",
    "mnames = list()\n",
    "mnames.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "mnames.append(\"svm.SVC(probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "mnames.append(\"AdaBoostClassifier()\")\n",
    "#mnames.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "#mnames.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    \n",
    "    print(test_pred_labels[0:13])\n",
    "    print(\"====\")\n",
    "    print(test_y[0:13])\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for v in test_pred_labels:\n",
    "        predictions_labels.append( int(v) )\n",
    "\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "    \n",
    "   \n",
    "    t1 = time.clock() - t0\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+str(v_mode)+\",\"+mnames[idx]+\",\"+str(len(vocab))+\",\"+str(vector_size)+\",\"+str(t1)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709d6e",
   "metadata": {},
   "source": [
    "## SMOTE and 2000 KBest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c58acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\n",
      "\n",
      "\n",
      "Most common: [('ن_ا', 8879), ('ا_ن', 8604)]\n",
      "Vocabulary Size: 58043\n",
      "A sample doc and its y: ['ل_و', 'و_ئ', 'ئ_ر', 'ر_ت', 'ت_ھ', 'ھ_ا', 'ا_ن', 'ن_ہ', 'ہ_م', 'م_ن', 'ن_ڈ', 'ڈ_ہ', 'ہ_ح', 'ح_د', 'د_و', 'و_د', 'د_گ', 'گ_و', 'و_س', 'س_م', 'م_ف', 'ف_ا', 'ا_ئ', 'ئ_ر', 'ر_ن', 'ن_گ', 'گ_پ', 'پ_ا', 'ا_ن', 'ن_چ', 'چ_ا', 'ا_ف', 'ف_ر', 'ر_ا', 'ا_د', 'د_ا', 'ا_ف', 'ف_ر', 'ر_ا', 'ا_د', 'د_ت', 'ت_ع', 'ع_ل', 'ل_ق', 'ق_خ', 'خ_ا', 'ا_ن', 'ن_د', 'د_ا', 'ا_ن', 'لوئر', 'تھانہ', 'منڈہ', 'حدود', 'گوسم', 'فائرنگ', 'پانچ', 'افراد', 'افراد', 'تعلق', 'خاندان', 'لوئر_تھانہ', 'تھانہ_منڈہ', 'منڈہ_حدود', 'حدود_گوسم', 'گوسم_فائرنگ', 'فائرنگ_پانچ', 'پانچ_افراد', 'افراد_افراد', 'افراد_تعلق', 'تعلق_خاندان'] 0\n",
      "selected best 2000 : 2000\n",
      "Sample features: [0.         2.2600628  3.93255152 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Counter({'0': 4929, '1': 4929})\n",
      "['0' '0' '0' '1' '1' '0' '0' '1' '0' '0' '0' '0' '0']\n",
      "====\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1,1,2,all,tfidf,svm.SVC(kernel='sigmoid', probability=True),58043,38277,958.5135239,41.757049891540134\n",
      "=========================================================================\n",
      "['0' '0' '0' '1' '1' '0' '0' '1' '0' '0' '0' '0' '0']\n",
      "====\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1,1,2,all,tfidf,svm.SVC(probability=True),58043,38277,1856.6144462,42.42068155111633\n",
      "=========================================================================\n",
      "['0' '0' '0' '1' '1' '0' '0' '1' '0' '0' '0' '0' '0']\n",
      "====\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1,1,2,all,tfidf,svm.SVC(kernel='poly',degree=1, probability=True),58043,38277,2746.8856108,42.5997425997426\n",
      "=========================================================================\n",
      "['0' '0' '0' '1' '1' '0' '0' '1' '0' '0' '0' '1' '0']\n",
      "====\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1,1,2,all,tfidf,AdaBoostClassifier(),58043,38277,2769.6997208999996,39.74603174603175\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "#To start with Applying bag of words model with normal models\n",
    "v_stw=1 \n",
    "v_lma=1\n",
    "v_ngram=2\n",
    "v_how = \"all\"\n",
    "v_mode=\"tfidf\"\n",
    "v_ch_ngram=2\n",
    "v_ch_how=\"exact\"\n",
    "\n",
    "print(\"Stopwords,Lemmatization,NGram,How NGram,Mode,Model,Vocab size,Vector size,Time elapsed,F1\\n\")                        \n",
    "t0 = time.clock()\n",
    "print()\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduThreat/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduThreat/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "table = process_dataset(load_file('UrduThreat/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "keys = extract_ids_from_dict(table)\n",
    "#print(keys[0]) \n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_docs)\n",
    "X = tokenizer.texts_to_matrix(X_docs, mode=v_mode)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vector_size = len(tokenizer.word_index) + 1\n",
    "#print(\"sentence vector size:\", vector_size)\n",
    "                            \n",
    "\n",
    "y = array(y)\n",
    "n_words = X.shape[1]\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "testcsv = load_file('UrduThreat/test.csv')       \n",
    "test_ans_slpit_csv = load_file('UrduThreat/test_answ_and_split.csv')\n",
    "\n",
    "(test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                           stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                           ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "\n",
    "#print(test_x[])\n",
    "\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(test_x, mode=v_mode)\n",
    "#print(Xtest)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "reduced_f = 2000 #10000\n",
    "selector = SelectKBest(chi2, k=reduced_f).fit(X, y) \n",
    "train_x = selector.transform(X)\n",
    "print(\"selected best\",reduced_f,\":\", len(train_x[0]))\n",
    "print(\"Sample features:\", train_x[0][:10])\n",
    "\n",
    "Xtest = selector.transform(Xtest)\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#models\n",
    "models =list()\n",
    "models.append(svm.SVC(kernel='sigmoid', probability=True))\n",
    "models.append(svm.SVC(probability=True))\n",
    "models.append(svm.SVC(kernel='poly',degree=1, probability=True))\n",
    "models.append(AdaBoostClassifier())\n",
    "#models.append(svm.SVC(kernel='poly',degree=2, probability=True))\n",
    "#models.append(svm.SVC(kernel='poly',degree=3, probability=True))\n",
    "\n",
    "\n",
    "mnames = list()\n",
    "mnames.append(\"svm.SVC(kernel='sigmoid', probability=True)\")\n",
    "mnames.append(\"svm.SVC(probability=True)\")\n",
    "mnames.append(\"svm.SVC(kernel='poly',degree=1, probability=True)\")\n",
    "mnames.append(\"AdaBoostClassifier()\")\n",
    "#mnames.append(\"svm.SVC(kernel='poly',degree=2, probability=True)\")\n",
    "#mnames.append(\"svm.SVC(kernel='poly',degree=3, probability=True)\")\n",
    "\n",
    "idx=0\n",
    "for model in models:\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    test_pred_labels = model.predict(Xtest)\n",
    "    \n",
    "    print(test_pred_labels[0:13])\n",
    "    print(\"====\")\n",
    "    print(test_y[0:13])\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for v in test_pred_labels:\n",
    "        predictions_labels.append( int(v) )\n",
    "\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "    strf1 = str(f1*100)\n",
    "    \n",
    "   \n",
    "    t1 = time.clock() - t0\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"+str(v_how)+\",\"+str(v_mode)+\",\"+mnames[idx]+\",\"+str(len(vocab))+\",\"+str(vector_size)+\",\"+str(t1)+\",\"+strf1\n",
    "    idx+=1\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b1ea3",
   "metadata": {},
   "source": [
    "### Deep Convolutional Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3247ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Most common: [('کشمیر', 964), ('پاکستان', 874)]\n",
      "train-test Vocabulary Size: 10968\n",
      "A sample doc and its y: ['لوئر', 'تھانہ', 'منڈہ', 'حدود', 'گوسم', 'فائرنگ', 'پانچ', 'افراد', 'افراد', 'تعلق', 'خاندان'] 0\n",
      "Max document length: 18\n",
      "Vocabulary size: 8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhumayoun\\Anaconda3\\envs\\mle\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.4508 - accuracy: 0.8215\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.2904 - accuracy: 0.8745\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.1312 - accuracy: 0.9480\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.0773 - accuracy: 0.9718\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.0498 - accuracy: 0.9827\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.0383 - accuracy: 0.9868\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.0380 - accuracy: 0.9878\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 50s 8ms/step - loss: 0.0255 - accuracy: 0.9913\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.0273 - accuracy: 0.9905\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.0222 - accuracy: 0.9917\n",
      "1,1,1,all,NA,\"Very Deep CNN\",10968,8068,532.4785228000001,35.328898743532896\n",
      "=========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhumayoun\\Anaconda3\\envs\\mle\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.4579 - accuracy: 0.8198\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 55s 9ms/step - loss: 0.3058 - accuracy: 0.8617\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.1437 - accuracy: 0.9432\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 52s 9ms/step - loss: 0.0725 - accuracy: 0.9738\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.0515 - accuracy: 0.9823\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - 52s 9ms/step - loss: 0.0395 - accuracy: 0.9860\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 53s 9ms/step - loss: 0.0298 - accuracy: 0.9895\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.0281 - accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 56s 9ms/step - loss: 0.0254 - accuracy: 0.9910\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 57s 9ms/step - loss: 0.0187 - accuracy: 0.9923\n",
      "1,1,1,all,NA,\"Very Deep CNN\",10968,8068,1073.8453962000003,33.33333333333333\n",
      "=========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhumayoun\\Anaconda3\\envs\\mle\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 55s 9ms/step - loss: 0.4546 - accuracy: 0.8207\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 58s 10ms/step - loss: 0.2825 - accuracy: 0.8788\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 54s 9ms/step - loss: 0.1280 - accuracy: 0.9518\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 51s 9ms/step - loss: 0.0736 - accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 56s 9ms/step - loss: 0.0485 - accuracy: 0.9827\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 56s 9ms/step - loss: 0.0408 - accuracy: 0.9858\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 60s 10ms/step - loss: 0.0357 - accuracy: 0.9885\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 55s 9ms/step - loss: 0.0313 - accuracy: 0.9880\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 55s 9ms/step - loss: 0.0239 - accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 51s 8ms/step - loss: 0.0248 - accuracy: 0.9912\n",
      "1,1,1,all,NA,\"Very Deep CNN\",10968,8068,1626.5740563,35.46511627906976\n",
      "=========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhumayoun\\Anaconda3\\envs\\mle\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.4540 - accuracy: 0.8210\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.2925 - accuracy: 0.8698\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.1354 - accuracy: 0.9465\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 48s 8ms/step - loss: 0.0744 - accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.0503 - accuracy: 0.9813\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.0390 - accuracy: 0.9862\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 48s 8ms/step - loss: 0.0297 - accuracy: 0.9873\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 48s 8ms/step - loss: 0.0274 - accuracy: 0.9910\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 49s 8ms/step - loss: 0.0203 - accuracy: 0.9925\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 48s 8ms/step - loss: 0.0225 - accuracy: 0.9915\n",
      "1,1,1,all,NA,\"Very Deep CNN\",10968,8068,2115.9057271,32.263242375601926\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy import array\n",
    "import numpy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s) for s in lines])\n",
    "\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length,  padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    \n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(32, 1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D()(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(32, 2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D()(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(32, 3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D()(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # channel 4\n",
    "    inputs4 = Input(shape=(length,))\n",
    "    embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "    conv4 = Conv1D(32, 4, activation='relu')(embedding4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling1D()(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3, flat4])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    #model.summary()\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model\n",
    "\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = None\n",
    "\n",
    "t0 = time.clock()\n",
    "print()\n",
    "print()\n",
    "\n",
    "# define vocab  \n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "vocab = Counter()\n",
    "# add all text to vocab\n",
    "add_doc_to_vocab('UrduThreat/train.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "add_doc_to_vocab('UrduThreat/test.csv', vocab, \n",
    "                 stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how, \n",
    "                 ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "                            \n",
    "print(\"Most common:\", vocab.most_common(2))\n",
    "vocab = set(vocab)\n",
    "print(\"train-test Vocabulary Size:\", len(vocab))\n",
    "                            \n",
    "table = process_dataset(load_file('UrduThreat/train.csv'), vocab, \n",
    "                        train=True, stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                        ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "X_docs, y = extract_text_label_from_dict(table)\n",
    "print(\"A sample doc and its y:\", X_docs[0], y[0])\n",
    "keys = extract_ids_from_dict(table)\n",
    "y = array(y)\n",
    "\n",
    "## create tokenizer\n",
    "tokenizer = create_tokenizer(X_docs)\n",
    "# calculate max document length\n",
    "length = max_length(X_docs)\n",
    "## load all training reviews\n",
    "X = encode_text(tokenizer, X_docs, length)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_x = X\n",
    "\n",
    "# calculate max document length\n",
    "length = max_length(train_x)\n",
    "print('Max document length: %d' % length)\n",
    "    \n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = train_x\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "#oversample = SMOTE()\n",
    "#X, y = oversample.fit_resample(trainX, y)\n",
    "# summarize the new class distribution\n",
    "#counter = Counter(y)\n",
    "#print(counter)\n",
    "X=trainX\n",
    "\n",
    "def deepCNN():\n",
    "    # define model\n",
    "    model = define_model(length, vocab_size)\n",
    "    # fit model\n",
    "    model.fit([X,X,X,X], y, epochs=10, batch_size=7)\n",
    "\n",
    "    # load all the test comments from test.csv\n",
    "    testcsv = load_file('UrduThreat/test.csv')       \n",
    "    test_ans_slpit_csv = load_file('UrduThreat/test_answ_and_split.csv')\n",
    "\n",
    "    (test_x, test_y) = mk_test(test_ans_slpit_csv, testcsv, vocab, \n",
    "                               stw=v_stw, lma=v_lma, ngram=v_ngram, how=v_how,\n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "    Xtest = encode_text(tokenizer, test_x, length)\n",
    "    test_y = array(test_y)\n",
    "\n",
    "    # make probability predictions with the model\n",
    "    predictions_scores = model.predict([Xtest,Xtest,Xtest,Xtest])\n",
    "\n",
    "\n",
    "    predictions_labels = list()\n",
    "    for score in predictions_scores:\n",
    "        if score>=0.5:\n",
    "            predictions_labels.append( int(1) )\n",
    "        else:\n",
    "            predictions_labels.append( int(0) )\n",
    "\n",
    "    f1 = f1_score(test_y, predictions_labels)\n",
    "\n",
    "    strf1 = str(f1*100)\n",
    "\n",
    "    t1 = time.clock() - t0\n",
    "\n",
    "    msg = str(v_stw)+\",\"+str(v_lma)+\",\"+str(v_ngram)+\",\"\n",
    "    msg+= str(v_how)+\",\"+\"NA\"+\",\\\"Very Deep CNN\\\",\"\n",
    "    msg+= str(len(vocab))+\",\"+str(vocab_size)+\",\"\n",
    "    msg+= str(t1)+\",\"+strf1\n",
    "\n",
    "    print(msg)\n",
    "    print(\"=========================================================================\")\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    deepCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed884cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82248f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under this all existing code before sept 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d733b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "url='samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrduThreat.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "#print(len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduThreat/train.csv'), vocab)\n",
    "\n",
    "X_docs_train = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "ytrain = extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1)\n",
    "\n",
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# decision tree evaluated on imbalanced dataset with SMOTE oversampling\n",
    "from numpy import mean\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='poly', probability=True).fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduThreat/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+str(float(test_pred_scores[i]))+'\\n'\n",
    "    \n",
    "file = open('UrduThreat/results/UrduThreat_results_9.csv', 'w')\n",
    "file.write(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59b5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c767b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078eabdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14854b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624167f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fc372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b931a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "url='samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrduThreat.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "#print(len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduThreat/train.csv'), vocab)\n",
    "\n",
    "X_docs_train = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "ytrain = extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1)\n",
    "\n",
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e860f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#print(imblearn.__version__)\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "# decision tree evaluated on imbalanced dataset with SMOTE oversampling\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "models=[DecisionTreeClassifier(), \n",
    "        AdaBoostClassifier(n_estimators=800, random_state = 1),\n",
    "        GaussianNB(),\n",
    "        RandomForestClassifier(max_depth=2, random_state=0),\n",
    "        LogisticRegression(random_state=0),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        svm.SVC(kernel='linear'),\n",
    "        svm.SVC(kernel='poly'),\n",
    "        svm.SVC(kernel='sigmoid'),\n",
    "        svm.SVC()\n",
    "       ]\n",
    "labels=[\n",
    "        'DecisionTreeClassifier', \n",
    "        'AdaBoostClassifier',\n",
    "        'GaussianNB()',\n",
    "        'RandomForestClassifier',\n",
    "        'LogisticRegression',\n",
    "        'KNeighborsClassifier',\n",
    "        'svm linear',\n",
    "        'svm poly',\n",
    "        'svm sigmoid',\n",
    "        'svm.SVC()'\n",
    "    ]\n",
    "c=0\n",
    "for model in models:\n",
    "    print(labels[c])\n",
    "    c=c+1\n",
    "    # evaluate pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    print('Mean ROC AUC: %.3f' % mean(scores))\n",
    "    scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
    "    print('Mean f1: %.3f' % mean(scores))\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "#Mean ROC AUC: 0.783\n",
    "#Mean f1: 0.791\n",
    "#AdaBoostClassifier\n",
    "#Mean ROC AUC: 0.901\n",
    "#Mean f1: 0.829\n",
    "#GaussianNB()\n",
    "#Mean ROC AUC: 0.777\n",
    "#Mean f1: 0.711\n",
    "#RandomForestClassifier\n",
    "#Mean ROC AUC: 0.803\n",
    "#Mean f1: 0.735\n",
    "#LogisticRegression\n",
    "#Mean ROC AUC: 0.816\n",
    "#Mean f1: 0.755\n",
    "#KNeighborsClassifier\n",
    "#Mean ROC AUC: 0.763\n",
    "#Mean f1: 0.738\n",
    "#svm linear\n",
    "#Mean ROC AUC: 0.813\n",
    "#Mean f1: 0.763\n",
    "#svm poly\n",
    "#Mean ROC AUC: 0.955\n",
    "#Mean f1: 0.892\n",
    "#svm sigmoid\n",
    "#Mean ROC AUC: 0.644\n",
    "#Mean f1: 0.623\n",
    "#svm.SVC()\n",
    "#Mean ROC AUC: 0.943\n",
    "#Mean f1: 0.877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db47da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borderline-SMOTE with SVM for imbalanced dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# define dataset\n",
    "X, y = train_x, train_y\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SVMSMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36532e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d80c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "url='samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrduThreat.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "#print(len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduThreat/train.csv'), vocab)\n",
    "\n",
    "X_docs_train = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "ytrain = extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1)\n",
    "\n",
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# decision tree evaluated on imbalanced dataset with SMOTE oversampling\n",
    "from numpy import mean\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='poly', probability=True).fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduThreat/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+str(float(test_pred_scores[i]))+'\\n'\n",
    "    \n",
    "file = open('UrduThreat/results/UrduThreat_results_9.csv', 'w')\n",
    "file.write(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "url='samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrduThreat.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "#print(len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduThreat/train.csv'), vocab)\n",
    "\n",
    "X_docs_train = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "ytrain = extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1)\n",
    "\n",
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# decision tree evaluated on imbalanced dataset with SMOTE oversampling\n",
    "from numpy import mean\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "model = svm.SVC(probability=True).fit(X,y)\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduThreat/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+str(float(test_pred_scores[i]))+'\\n'\n",
    "    \n",
    "file = open('UrduThreat/results/UrduThreat_results_10.csv', 'w')\n",
    "file.write(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469db129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f720fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab417038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e551a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d69c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2513ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ceea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b0be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ed98b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "url='samarh-urduvec/urduvec_140M_100K_300d.bin'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabUrduThreat.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"size of vocab:\", len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "lines_0, lines_1 = process_dataset(load_file('UrduThreat/train.csv'), vocab)\n",
    "\n",
    "X_docs_train = extract_text_from_dict(lines_0) + extract_text_from_dict(lines_1)\n",
    "ytrain = extract_label_from_dict(lines_0) + extract_label_from_dict(lines_1)\n",
    "\n",
    "print(X_docs_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_train:\n",
    "    print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            print(\"len:\", len(word_vec))\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    print(\"docs_vectors: \", docs_vectors)\n",
    "    break\n",
    "    \n",
    "print(docs_vectors.shape)\n",
    "\n",
    "pd.isnull(docs_vectors).sum().sum()\n",
    "\n",
    "docs_vectors['Threat'] = ytrain\n",
    "docs_vectors = docs_vectors.fillna(0)\n",
    "\n",
    "\n",
    "train_x = docs_vectors.drop('Threat', axis = 1)\n",
    "train_y = docs_vectors['Threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=['لوئر', 'دیر', 'تھانہ', 'حدود', 'گوسم']\n",
    "temp = pd.DataFrame()\n",
    "for word in doc:\n",
    "    try:\n",
    "        word_vec = embeddings[word]\n",
    "        print(\"len:\", len(word_vec))\n",
    "        temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "    except:\n",
    "        pass\n",
    "    doc_vector = temp.mean() \n",
    "\n",
    "print(temp[0])\n",
    "print(temp.mean())    \n",
    "#doc_vector\n",
    "#word_vec = embeddings['تھانہ']\n",
    "#print(word_vec)\n",
    "x =[-0.026435, 0.017119, -0.263006, 0.332762]\n",
    "xx = sum(x)/len(x)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_x, train_y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "# decision tree evaluated on imbalanced dataset with SMOTE oversampling\n",
    "from numpy import mean\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='poly', probability=True).fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "# load all the test comments from test.csv\n",
    "comments = process_test_dataset(load_file('UrduThreat/test.csv'), vocab)\n",
    "\n",
    "X_docs_test = extract_text_from_dict(comments)\n",
    "\n",
    "tdocs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "for doc in X_docs_test:\n",
    "    #print(doc)\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    tdoc_vector = temp.mean() \n",
    "    tdocs_vectors = tdocs_vectors.append(tdoc_vector, ignore_index = True)\n",
    "    \n",
    "print(tdocs_vectors.shape)\n",
    "\n",
    "pd.isnull(tdocs_vectors).sum().sum()\n",
    "\n",
    "tdocs_vectors = tdocs_vectors.fillna(0)\n",
    "\n",
    "#tdocs_vectors\n",
    "\n",
    "Xtest = tdocs_vectors\n",
    "\n",
    "\n",
    "# make probability predictions with the model\n",
    "test_pred_labels = model.predict(Xtest)\n",
    "test_pred_scr = model.predict_proba(Xtest)\n",
    "\n",
    "test_pred_scores = list()\n",
    "for i in test_pred_labels:\n",
    "    test_pred_scores.append(max(test_pred_scr.tolist()[i]))\n",
    "\n",
    "ids = extract_id_from_dict(comments)\n",
    "\n",
    "results=\"id,target,score\\n\"#+os.linesep\n",
    "for i in range(0, len(test_pred_scores)):\n",
    "    results += str(ids[i])+\",\"+str(test_pred_labels[i])+\",\"+str(float(test_pred_scores[i]))+'\\n'\n",
    "    \n",
    "#file = open('UrduThreat/results/UrduThreat_results_9.csv', 'w')\n",
    "#file.write(results)\n",
    "#file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
